{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate and Toxic Comment Detection \n",
    "\n",
    "**DataLab USAL - Project 2019/2020** \n",
    "\n",
    "### 01. Corpus Exploration\n",
    "\n",
    "Este primer Notebook servirá como primera toma de contacto. Cargaremos los tweets y daremos el formato adecuado a la información de la que disponemos. Además, realizaremos un análisis exploratorio básico sobre las variables con algunos gráficos descriptivos.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "#### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# misc\n",
    "import gzip\n",
    "import logging\n",
    "import codecs\n",
    "global lcode\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# nlp\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "import itertools\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "import networkx\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def initialize_vars():\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "    from warnings import filterwarnings\n",
    "    \n",
    "    \n",
    "    # pandas parameters\n",
    "    pd.set_option('display.max_columns', None)\n",
    "\n",
    "    # seaborn parameters\n",
    "    sns.set(font_scale=1.5)\n",
    "    sns.set_style(\"whitegrid\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = \"../DATA\"\n",
    "\n",
    "initialize_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Dataset\n",
    "\n",
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 s, sys: 800 ms, total: 15.3 s\n",
      "Wall time: 19.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def load_dataframe(\n",
    "        get_filename_fn=lambda i: f\"file_{i}.csv\",\n",
    "        csv_column_names=[],\n",
    "        selected_columns=None,\n",
    "        file_range=(0, 1),\n",
    "        encoding='utf-8',\n",
    "        sep='\\t'\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Load the dataframe files in range(`file_range[0]`, `file_range[1]`) \n",
    "    using the function `get_filename_fn` to retrieve filenames.\n",
    "    \n",
    "    :param get_filename_fn:    Function to retrieve filename based on an index.\n",
    "    :param file_range:         Tuple (Starting index, End Index).\n",
    "    :param csv_column_names:   All column names (ordered) present in CSV files.\n",
    "    :param selected_columns:   Selected columns to read (only these will be present).\n",
    "    :param encoding:           Encoding for CSV files.\n",
    "    :param sep:                Column separator\n",
    "    :returns:                  Full dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Set dataframes generator\n",
    "    dataframes = (pd.read_csv(\n",
    "        filepath_or_buffer=get_filename_fn(i), \n",
    "        sep=sep, \n",
    "        names=csv_column_names, \n",
    "        encoding=encoding,\n",
    "        usecols=selected_columns if selected_columns is not None else csv_column_names\n",
    "    ) for i in range(file_range[0], file_range[1]))\n",
    "\n",
    "    # Load and concat\n",
    "    return pd.concat(dataframes).astype(str)\n",
    "    \n",
    "\n",
    "tweets = load_dataframe(\n",
    "    get_filename_fn=lambda i: f\"{data_folder}/streaming_odio_politicas_{i}.txt\",\n",
    "    file_range=(0, 9),\n",
    "    csv_column_names=[\"id\", \"date\", \"user\", \"tweet\", \"via\", \"user_id\", \"followers\", \"following\", \n",
    "                      \"statuses\", \"loc\", \"link\", \"col12\", \"col13\", \"user_bio\", \"col15\", \"col16\", \n",
    "                      \"col17\", \"tweet_type\", \"col19\", \"politician_mention\", \"col21\", \"politician_rt\", \n",
    "                      \"col23\", \"col24\", \"hashtag\", \"col26\",\"count_creation\", \"col28\", \"col29\", \"col30\"],\n",
    "    selected_columns=[\"date\", \"user\", \"tweet\", \"via\", \"followers\", \"following\", \"loc\", \"link\", \"col12\", \n",
    "                      \"user_bio\", \"col17\", \"tweet_type\", \"politician_mention\", \"politician_rt\", \"col24\", \n",
    "                      \"hashtag\", \"count_creation\", \"col28\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>tweet</th>\n",
       "      <th>via</th>\n",
       "      <th>followers</th>\n",
       "      <th>following</th>\n",
       "      <th>loc</th>\n",
       "      <th>link</th>\n",
       "      <th>col12</th>\n",
       "      <th>user_bio</th>\n",
       "      <th>col17</th>\n",
       "      <th>tweet_type</th>\n",
       "      <th>politician_mention</th>\n",
       "      <th>politician_rt</th>\n",
       "      <th>col24</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>count_creation</th>\n",
       "      <th>col28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-05 17:10:01</td>\n",
       "      <td>@mikemit87350572</td>\n",
       "      <td>@InesArrimadas Si quieres los votos de VOX vas...</td>\n",
       "      <td>via=Twitter for Android</td>\n",
       "      <td>followers=22</td>\n",
       "      <td>following=50</td>\n",
       "      <td>loc=None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reply</td>\n",
       "      <td>InesArrimadas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2018-12-13 18:45:18</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-05 17:10:22</td>\n",
       "      <td>@DulantziNabar</td>\n",
       "      <td>RT @Irene_Montero_: En Unidas Podemos todos so...</td>\n",
       "      <td>via=Twitter for Android</td>\n",
       "      <td>followers=2028</td>\n",
       "      <td>following=1911</td>\n",
       "      <td>loc=Bilbao</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No se llega al alba sino por el sendero de la ...</td>\n",
       "      <td>None</td>\n",
       "      <td>RT</td>\n",
       "      <td>None</td>\n",
       "      <td>Irene_Montero_</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2017-06-10 17:57:06</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-05 17:10:22</td>\n",
       "      <td>@CdpdDavid</td>\n",
       "      <td>RT @InesArrimadas: Sánchez ya ha elegido a sus...</td>\n",
       "      <td>via=Twitter for Android</td>\n",
       "      <td>followers=1354</td>\n",
       "      <td>following=1498</td>\n",
       "      <td>loc=None</td>\n",
       "      <td>https://www.elespanol.com/opinion/tribunas/201...</td>\n",
       "      <td>None</td>\n",
       "      <td>Cineasta Guionista y escritor. Apasionado d la...</td>\n",
       "      <td>None</td>\n",
       "      <td>RT</td>\n",
       "      <td>None</td>\n",
       "      <td>InesArrimadas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-05-12 14:24:08</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-05 17:10:23</td>\n",
       "      <td>@Reinaraf1962</td>\n",
       "      <td>RT @Irene_Montero_: En Unidas Podemos todos so...</td>\n",
       "      <td>via=Twitter for iPhone</td>\n",
       "      <td>followers=29</td>\n",
       "      <td>following=65</td>\n",
       "      <td>loc=None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>RT</td>\n",
       "      <td>None</td>\n",
       "      <td>Irene_Montero_</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2015-01-12 17:35:16</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-05 17:10:27</td>\n",
       "      <td>@juannquillo</td>\n",
       "      <td>@sanchezcastejon Lo que España necesita es de ...</td>\n",
       "      <td>via=Twitter for Android</td>\n",
       "      <td>followers=50</td>\n",
       "      <td>following=171</td>\n",
       "      <td>loc=None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>reply</td>\n",
       "      <td>sanchezcastejon</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2011-05-19 09:58:20</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  date              user  \\\n",
       "0  2019-07-05 17:10:01  @mikemit87350572   \n",
       "1  2019-07-05 17:10:22    @DulantziNabar   \n",
       "2  2019-07-05 17:10:22        @CdpdDavid   \n",
       "3  2019-07-05 17:10:23     @Reinaraf1962   \n",
       "4  2019-07-05 17:10:27      @juannquillo   \n",
       "\n",
       "                                               tweet                      via  \\\n",
       "0  @InesArrimadas Si quieres los votos de VOX vas...  via=Twitter for Android   \n",
       "1  RT @Irene_Montero_: En Unidas Podemos todos so...  via=Twitter for Android   \n",
       "2  RT @InesArrimadas: Sánchez ya ha elegido a sus...  via=Twitter for Android   \n",
       "3  RT @Irene_Montero_: En Unidas Podemos todos so...   via=Twitter for iPhone   \n",
       "4  @sanchezcastejon Lo que España necesita es de ...  via=Twitter for Android   \n",
       "\n",
       "        followers       following         loc  \\\n",
       "0    followers=22    following=50    loc=None   \n",
       "1  followers=2028  following=1911  loc=Bilbao   \n",
       "2  followers=1354  following=1498    loc=None   \n",
       "3    followers=29    following=65    loc=None   \n",
       "4    followers=50   following=171    loc=None   \n",
       "\n",
       "                                                link col12  \\\n",
       "0                                               None  None   \n",
       "1                                               None  None   \n",
       "2  https://www.elespanol.com/opinion/tribunas/201...  None   \n",
       "3                                               None  None   \n",
       "4                                               None  None   \n",
       "\n",
       "                                            user_bio col17 tweet_type  \\\n",
       "0                                               None  None      reply   \n",
       "1  No se llega al alba sino por el sendero de la ...  None         RT   \n",
       "2  Cineasta Guionista y escritor. Apasionado d la...  None         RT   \n",
       "3                                               None  None         RT   \n",
       "4                                               None  None      reply   \n",
       "\n",
       "  politician_mention   politician_rt col24 hashtag       count_creation  col28  \n",
       "0      InesArrimadas            None  None    None  2018-12-13 18:45:18  False  \n",
       "1               None  Irene_Montero_  None    None  2017-06-10 17:57:06  False  \n",
       "2               None   InesArrimadas  None    None  2015-05-12 14:24:08  False  \n",
       "3               None  Irene_Montero_  None    None  2015-01-12 17:35:16  False  \n",
       "4    sanchezcastejon            None  None    None  2011-05-19 09:58:20  False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new column RT, True if it is an RT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.71 s, sys: 104 ms, total: 1.82 s\n",
      "Wall time: 1.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "\n",
    "def create_rt_and_rt_source():\n",
    "    \n",
    "    tweets['RT'] = tweets[\"tweet_type\"] == \"RT\"\n",
    "\n",
    "    # Create new column RT_source, if RT, with retweeted account\n",
    "    # Search for \"RT @something\" in the tweet and selects the `@something` part with `rt_source` group\n",
    "    regex = re.compile(\"^RT +?(?P<rt_source>@[a-zA-Z_0-9]+)\")\n",
    "    \n",
    "    def select_rt_source(text):\n",
    "        if regex.search(text):\n",
    "            regex.search(text).group('rt_source')\n",
    "        else:\n",
    "            ''\n",
    "        \n",
    "    tweets.loc[tweets['RT'], 'RT_source'] = tweets[tweets['RT']]['tweet'].apply(select_rt_source)\n",
    "\n",
    "    # If not RT, RT_source = ''\n",
    "    #tweets.loc[np.logical_not(tweets['RT']), 'RT_source'] = ''\n",
    "\n",
    "    \n",
    "create_rt_and_rt_source()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vars type casting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"date\"] = pd.to_datetime(tweets[\"date\"], format=\"%Y-%m-%d %H:%M:%S\", errors=\"coerce\")\n",
    "tweets[\"followers\"] = pd.to_numeric(tweets[\"followers\"], errors=\"coerce\")\n",
    "tweets[\"following\"] = pd.to_numeric(tweets[\"following\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 236 ms, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def clean_dataset_column(column_array):\n",
    "    import re\n",
    "    from unidecode import unidecode\n",
    "    \n",
    "    # Individual regexes to match and clean\n",
    "    remove_regexes = [\n",
    "        \"[\\]\\[\\\"?!,. ]+\",              # Sequence of whitespace\n",
    "        \"(RT )?@[_A-Za-z0-9]+:?\",      # '@' mentions, RT '@' mentions\n",
    "        \"[a-z]+:\\/\\/[a-zA-Z0-9./_\\-]+\",# (http|https|any)://url links\n",
    "    ]\n",
    "\n",
    "    # Compile regex to speed up processing; join regexes with an 'or' and match any occurrence\n",
    "    regex = re.compile(f\"({'|'.join(remove_regexes)})+\")\n",
    "\n",
    "    # \n",
    "    clean_tweet_fn = lambda text: regex.sub(\" \", unidecode(text)).strip().lower()\n",
    "\n",
    "    # clean matched elements and set it to a dataset column\n",
    "    return column_array.apply(clean_tweet_fn)\n",
    "\n",
    "\n",
    "tweets['clean_tweet'] = clean_dataset_column(tweets['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 39.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.es.stop_words import STOP_WORDS as spacy_stop_words\n",
    "from unidecode import unidecode\n",
    "\n",
    "def get_stop_words_set():\n",
    "    \n",
    "    \n",
    "    # Download stopwords assets\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "    stopwords_languages = (\n",
    "        'spanish',\n",
    "        'english'\n",
    "    )\n",
    "    \n",
    "    custom_stop_words={\n",
    "        'q', 'k', 'tiene','vuestra','habiendo','han','tendremos','tenemos','hubisteis','hayas',\n",
    "        'habian','seas','muy','se','tienes','habidos','mia','tuviste','tengas','contra','tuvieseis',\n",
    "        'habiais','he','estariamos','estuvieron','para','fui','has','al','era','tenidos','nada',\n",
    "        'estados','tuvieran','teneis','sentidas','fueran','estare','en','sois','que','estuvieras',\n",
    "        'tendras','tuvieses','estado','erais','sus','si','seais','estuvieramos','hubimos','tuyas',\n",
    "        'esteis','quienes','ella','todos','sea','fueses','otra','tenido','mios','del','sean','tenia',\n",
    "        'mas','tambien','fueseis','ni','vuestro','habiamos','fueras','tu','que','sentida','antes',\n",
    "        'y','hube','tendrian','tu','estarian','este','tuvierais','os','tendre','no','fueramos','vosotras',\n",
    "        'ti','tendrias','seran','habriais','esta','estos','lo','estuviesemos','hubieses','eres',\n",
    "        'nuestra','hayamos','estaras','la','estamos','habria','somos','tendran','yo','habras','habra',\n",
    "        'seria','hay','poco','tendriamos','por','el','pero','habrias','estuviste','sobre','otro',\n",
    "        'estaran','habeis','esos','tuvieron','hayais','cuando','estadas','estuvo','tenias','estemos',\n",
    "        'sintiendo','teniamos','habias','tuvo','hubiese','ha','vuestras','son','esto','nosotros',\n",
    "        'estaba','como','habre','habreis','mucho','habrian','vosotros','eras','fue','sentid','sera',\n",
    "        'mi','estas','ante','es','tuvieramos','mias','me','hubieras','tenian','esta','estuvieses',\n",
    "        'hubiesemos','tuve','fuese','estuvierais','los','hubieron','desde','fuiste','cual','estuve',\n",
    "        'tus','mis','ya','nuestros','tuviese','donde','su','eran','ese','serias','habidas','durante',\n",
    "        'habran','tengamos','tuvimos','le','estaban','seamos','estarias','un','algo','tengan','sereis',\n",
    "        'estabamos','tuviesemos','estara','hubieseis','serian','seriais','tenida','estabais','estuvimos',\n",
    "        'estuviesen','mio','fueron','siente','ellos','e','suya','estuviese','estuvieran','nosotras',\n",
    "        'estuvieseis','o','tienen','las','habriamos','estuviera','tengo','estando','muchos','tenidas',\n",
    "        'tendra','estaremos','el','hubo','estaria','sere','a','vuestros','hubierais','suyo','tuvisteis',\n",
    "        'otros','tuvieras','eramos','estes','habido','estoy','suyas','estabas','fuerais','algunas',\n",
    "        'esten','soy','otras','tanto','estariais','estareis','hasta','habremos','esa','hayan','mi',\n",
    "        'eso','quien','tened','estan','tuya','hubieran','entre','seriamos','unos','hubiera','porque',\n",
    "        'algunos','estas','fuera','estar','habida','fuesen','tendreis','con','tengais','suyos',\n",
    "        'este','estad','fuisteis','sentido','todo','hubieramos','estuvisteis','sin','seremos','tuyos',\n",
    "        'de','uno','les','nuestro','sentidos','teniais','hubiesen','hubiste','tuviesen','esas','tendria',\n",
    "        'seras','teniendo','estais','te','tuyo','fuimos','haya','fuesemos','habia','nuestras','tenga',\n",
    "        'una','tendriais','tuviera','hemos','nos','ellas','estada'\n",
    "    }\n",
    "\n",
    "    return {unidecode(word) for word in custom_stop_words | spacy_stop_words | set(stopwords.words(stopwords_languages))}\n",
    "\n",
    "\n",
    "stop_words = get_stop_words_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires `stop_words` and `tweets['clean_tweet']`\n",
    "\n",
    "def simple_tokenization():\n",
    "    tweets['tokens'] = tweets['clean_tweet'].apply(lambda text: [word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    \n",
    "simple_tokenization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for tweet_group in tweets['tokens']:\n",
    "    sentences.extend(tweet_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 40s, sys: 3.36 s, total: 7min 44s\n",
      "Wall time: 3min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Set values for various parameters\n",
    "num_features = 200    # Word vector dimensionality\n",
    "min_word_count = 3    # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 6           # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "# Initialize and train the model\n",
    "W2Vmodel = Word2Vec(sentences=sentences,\n",
    "                    sg=1,\n",
    "                    hs=0,\n",
    "                    workers=num_workers,\n",
    "                    size=num_features,\n",
    "                    min_count=min_word_count,\n",
    "                    window=context,\n",
    "                    sample=downsampling,\n",
    "                    negative=5,\n",
    "                    iter=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['W2Vmodel.pkl']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(W2Vmodel, 'W2Vmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2Vmodel = joblib.load('W2Vmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_features(w2v_model, sentence_group):\n",
    "    \"\"\" Transform a sentence_group (containing multiple lists\n",
    "    of words) into a feature vector. It averages out all the\n",
    "    word vectors of the sentence_group.\n",
    "    \"\"\"\n",
    "    words = np.concatenate(sentence_group)  # words in tex\n",
    "    index2word_set = set(w2v_model.wv.vocab.keys())  # words known to model\n",
    "    \n",
    "    featureVec = np.zeros(w2v_model.vector_size, dtype=\"float32\")\n",
    "    \n",
    "    # Initialize a counter for number of words in a review\n",
    "    nwords = 0\n",
    "    # Loop over each word in the comment and, if it is in the model's vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            featureVec = np.add(featureVec, w2v_model[word])\n",
    "            nwords += 1.\n",
    "\n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords > 0:\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "137628",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-245b80158f83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msen_group\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_w2v_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2Vmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-245b80158f83>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(sen_group)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msen_group\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_w2v_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW2Vmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msen_group\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-23b40a196730>\u001b[0m in \u001b[0;36mget_w2v_features\u001b[0;34m(w2v_model, sentence_group)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mword\u001b[0m \u001b[0mvectors\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentence_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_group\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# words in tex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mindex2word_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# words known to model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    621\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2558\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2559\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2560\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2561\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2562\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine._get_loc_duplicates\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.index.Int64Engine._maybe_get_bool_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 137628"
     ]
    }
   ],
   "source": [
    "w2v_features = list(map(lambda sen_group: get_w2v_features(W2Vmodel, sen_group), [tweets['tokens']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_tweet_w2v = np.array(list(map(np.array, w2v_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_tweet_w2v.to_hdf('../DATA/M_tweet_w2v.h5', key='M_tweet_w2v', mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term frequency - Inverse document frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def create_tfidf_matrix(column_array):\n",
    "        \n",
    "    # Load spacy es_core_news_sm\n",
    "    # Install with `python -m spacy install es_core_news_sm` in your conda distribution\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    \n",
    "    # Discarded part of speech set\n",
    "    discarded_pos_set = {'PROPN', 'CONJ', 'ADP', 'DET'}\n",
    "    \n",
    "    # Test if token_is_valid\n",
    "    token_is_valid = lambda t: t.is_alpha and not t.is_stop and t.pos_ not in set_pos\n",
    "    \n",
    "    # Pick lemma_.lower() for token in text if token_is_valid\n",
    "    tokenizer = lambda text: (t.lemma_.lower() for t in nlp(text) if token_is_valid(t))\n",
    "    \n",
    "    # Define vectorizer parameters\n",
    "    vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=3, stop_words=stop_words,\n",
    "                                 use_idf=True, tokenizer=tokenizer, ngram_range=(1,3))\n",
    "\n",
    "     # Fit matrix\n",
    "    matrix = vectorizer.fit_transform(column_array)\n",
    "    \n",
    "    return matrix, vectorizer\n",
    "\n",
    "\n",
    "tfidf_matrix_train, tfidf_vectorizer = create_tfidf_matrix(tweets['clean_tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save csr_matrix data to a hdf5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load functions & variables\n",
    "\n",
    "class CSRMatrixHDF5FileHandler():\n",
    "    # Initialize class\n",
    "    def __init__(self, filename = 'csr_matrix.hd5', group='Mcsr'):\n",
    "        self.filename = filename\n",
    "        self.group = group\n",
    "    \n",
    "    def save(self, matrix):\n",
    "        \"\"\"Save a csr_matrix into `self.filename` hdf5 file\"\"\"\n",
    "        import h5py\n",
    "        with h5py.File(self.filename, 'w') as f:\n",
    "            g = f.create_group(self.group)\n",
    "            g.create_dataset('data',data=matrix.data)\n",
    "            g.create_dataset('indptr',data=matrix.indptr)\n",
    "            g.create_dataset('indices',data=matrix.indices)\n",
    "            g.attrs['shape'] = matrix.shape\n",
    "        \n",
    "    def load(self):\n",
    "        \"\"\"Load a csr_matrix from `self.filename` hdf5 file\"\"\"\n",
    "        import h5py\n",
    "        from scipy import sparse\n",
    "        with h5py.File(self.filename,'r') as f:\n",
    "            g = f[self.group]\n",
    "            return sparse.csr_matrix((g['data'][:], g['indices'][:], g['indptr'][:]), g.attrs['shape'])\n",
    "        \n",
    "\n",
    "# Load handler\n",
    "matrix_file_handler = CSRMatrixHDF5FileHandler(\n",
    "    filename = f\"{data_folder}/tfidf_matrix.hd5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save command\n",
    "# matrix_file_handler.save(tfidf_matrix_train)\n",
    "\n",
    "# Load command\n",
    "tfidf_matrix_train = matrix_file_handler.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Requires `tfidf_matrix_train`\n",
    "\n",
    "def graph_csr_matrix():\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(10, 80))\n",
    "    plt.spy(tfidf_matrix_train, markersize=0.05, precision=0.02)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "graph_csr_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtrar por tweets dirigidos a politicos (dir) y tweets escritos por politicos (col3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar por politico tanto en col3 como en Dir, ver cuantos escribe cada uno y cuantos recibe, ver las palabras más habituales.\n",
    "\n",
    "Filtrar los que son RT, obtener palabras más frecuentes tanto  RT en general, como por dirigido a politico y escrito por poltico. \n",
    "\n",
    "Filtramos los tweets escritos por los distintos politicos y obtenemos el número de tweets escritos y las palabras más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Requires the execution of `simple_tokenization`\n",
    "\n",
    "def create_counts():\n",
    "    from itertools import chain\n",
    "    from collections import Counter\n",
    "\n",
    "    return {\n",
    "        # Count by user\n",
    "#         'user': tweets.groupby('user').apply(lambda el: Counter(chain(*el['tokens']))),\n",
    "        # All words count\n",
    "        'global': Counter(chain(*tweets['tokens'])),\n",
    "        # RT with most common hastags\n",
    "        'rt_hashtag': Counter(tweets[(tweets[\"tweet_type\"] == \"RT\") & (tweets[\"hashtag\"] != 'None')][\"hashtag\"])\n",
    "    }\n",
    "\n",
    "\n",
    "counts = create_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def graph_word_frequencies():\n",
    "    import pandas as pd    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    politicians = [\"@InesArrimadas\", \"@Irene_Montero_\", \"@carmencalvo_\", \"@cayetanaAT\", \n",
    "               \"@Albert_Rivera\", \"@Pablo_Iglesias_\", \"@sanchezcastejon\", \"@pablocasado_\",\n",
    "               \"@Santi_ABASCAL\", \"@monasterioR\"]\n",
    "    \n",
    "    # Set this variable with as many graphs as you like\n",
    "    counts_to_graph=[\n",
    "        # (graph_title,    Counter)\n",
    "        ('Total',          counts['global']),\n",
    "        ('RT Hashtag',     counts['rt_hashtag']),\n",
    "    ]# + [(user, counts['user'][user]) for user in politicians]\n",
    "    \n",
    "    graph_columns = 2\n",
    "    graph_rows = len(counts_to_graph)//graph_columns\n",
    "    \n",
    "    df_counts = [(name, pd.DataFrame(count.most_common(15), columns=['words', 'count']\n",
    "    )) for name, count in counts_to_graph]\n",
    "    \n",
    "    fig, axs = plt.subplots(graph_rows, graph_columns, figsize=(20, 16 * graph_rows))\n",
    "\n",
    "    \n",
    "    # Plot horizontal bar graph\n",
    "    pos_range = [(i, j) for i in range(graph_rows) for j in range(graph_columns)]\n",
    "\n",
    "    for pos_x, pos_y in pos_range[:len(df_counts)]:       \n",
    "        title, frame = df_counts[2*pos_x+pos_y]\n",
    "        ax = axs[pos_x, pos_y] if graph_rows > 1 else axs[pos_y]\n",
    "        \n",
    "        ax.set_title(title)\n",
    "        \n",
    "        frame.sort_values(by='count').plot.barh(\n",
    "            x='words',\n",
    "            y='count',\n",
    "            ax=ax,\n",
    "            color=\"purple\"\n",
    "        )\n",
    "    \n",
    "    \n",
    "graph_word_frequencies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def create_all_delta_splits(\n",
    "        dataset=tweets, \n",
    "        c_date=column_names[1], \n",
    "        date_range = (date(2019, 7, 5), date(2019, 9, 13))\n",
    "    ):\n",
    "    \n",
    "    from datetime import date, timedelta\n",
    "\n",
    "    \n",
    "    # Select observations between two datetimes\n",
    "    def delta_range(date_range, date_delta):\n",
    "        \"\"\"Creates `date_ranges(start, end)` given an original date_range and delta time for each range\"\"\"\n",
    "        from math import ceil\n",
    "\n",
    "        total_delta = date_range[1] - date_range[0]\n",
    "        deltas_in_total = total_delta/date_delta\n",
    "        \n",
    "        return (\n",
    "            (date_range[0] + delta_idx * date_delta, date_range[0] + (delta_idx + 1) * date_delta) \n",
    "            for delta_idx in range(ceil(deltas_in_total))\n",
    "        )\n",
    "\n",
    "    \n",
    "    def create_delta_split(date_range, date_delta, dataset=dataset, c_date=c_date):\n",
    "        \"\"\"Create an split `dataset` array of date_ranges of size `date_delta` in the `dataset`[`c_date`] column\"\"\"\n",
    "        return [tweets[(tweets[c_date] >= str(start)) & (tweets[c_date] < str(end))] for start, end in delta_range(date_range, date_delta)]\n",
    "\n",
    "    \n",
    "    monthly_delta_split = create_delta_split(\n",
    "        date_range,\n",
    "        timedelta(days=30)\n",
    "    )\n",
    "\n",
    "    fortnightly_delta_split = create_delta_split(\n",
    "        date_range,\n",
    "        timedelta(days=15)\n",
    "    )\n",
    "\n",
    "    weekly_delta_split = create_delta_split(\n",
    "        date_range,\n",
    "        timedelta(days=7)\n",
    "    )\n",
    "\n",
    "    daily_delta_split = create_delta_split(\n",
    "        date_range,\n",
    "        timedelta(days=1)\n",
    "    )\n",
    "        \n",
    "    return monthly_delta_split, fortnightly_delta_split, weekly_delta_split, daily_delta_split\n",
    "\n",
    "\n",
    "# delta_size: month = 0, fortnight = 1, week = 2, day = 3\n",
    "# idx_delta: 0 = 1st, 1 = 2nd, 2 = 3rd, 3 = 4th, ...\n",
    "# Usage: dataset_delta_splits[delta_size][idx_delta]\n",
    "# Example: dataset_delta_splits[2][1] ==> second week in the date_range\n",
    "dataset_delta_splits = create_all_delta_splits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets by date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets_by_date():\n",
    "    import pandas as pd\n",
    "    \n",
    "\n",
    "    tweets_date = pd.to_datetime(tweets['date']).dt.floor('d')\n",
    "    \n",
    "    tweets_by_date = tweets_date.value_counts().rename_axis('date').reset_index(name='count')\n",
    "    tweets_by_date = tweets_by_date.sort_values(by=['date'])\n",
    "    \n",
    "    return tweets_by_date\n",
    "\n",
    "tweets_by_date = get_tweets_by_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph tweets_by_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_tweets_by_date():\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(tweets_by_date['date'], tweets_by_date['count'])\n",
    "    plt.xlabel('DATES')\n",
    "    plt.ylabel('TWEETS COUNT')\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "\n",
    "graph_tweets_by_date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_lemm_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = []\n",
    "    for token in doc:\n",
    "        if token.is_stop== False: # aprovechamos para eliminar ya las stopwords\n",
    "            if token.is_alpha== True: # Nos quedamos solo con los tokens que contienen letras \n",
    "                if token.pos_ not in ['PROPN', 'CONJ', 'ADP', 'DET']: # eliminamos nombres propios, conjunciones, determinantes\n",
    "                    lemmas.append(token.lemma_.lower())\n",
    "    return lemmas\n",
    "\n",
    "def tokenize_only_spacy(text):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if token.is_stop== False :\n",
    "            if token.is_alpha== True:\n",
    "                if token.pos_ not in ['PROPN', 'CONJ', 'ADP', 'DET']:\n",
    "                    tokens.append(token.text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "comentarios = list(tweets['clean_tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('es_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000, min_df=3, stop_words=stop_words,\n",
    "                                   use_idf=True, tokenizer=tokenize_and_lemm_spacy, ngram_range=(1,3))\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(comentarios) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.spy(tfidf_matrix, markersize=0.1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = tfidf_vectorizer.get_feature_names()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
